{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/gieunkwak/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gieunkwak/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec을 이용한 단어 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT야, simpsons 캐릭터 이름이 들어간 랜덤 문장 10개를 생성해줘\n",
    "\n",
    "sentences = [\"Homer Simpson forgot his lunch at home, so he had to buy a burger on his way to work.\",\n",
    "    \"Marge was busy knitting a new sweater for Bart's upcoming school play.\",\n",
    "    \"Lisa Simpson played a beautiful saxophone solo at the school concert.\",\n",
    "    \"Mr. Burns secretly plotted another scheme from his office at the Springfield Nuclear Power Plant.\",\n",
    "    \"Ned Flanders offered to help Homer fix the fence between their houses.\",\n",
    "    \"Bart Simpson tried a new prank at school, but it didn't go as planned.\",\n",
    "    \"Milhouse and Bart spent the afternoon playing video games and forgot to do their homework.\",\n",
    "    \"Maggie Simpson's adorable giggle filled the room as she played with her toys.\",\n",
    "    \"Apu had a busy day at the Kwik-E-Mart, dealing with a rush of customers.\",\n",
    "    \"Krusty the Clown decided to change his show a bit to attract a new audience.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "# get rid of stopwords, lower case\n",
    "\n",
    "sentences = [s.lower().replace(\".\", \"\").split(\" \") for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['homer',\n",
       " 'simpson',\n",
       " 'forgot',\n",
       " 'his',\n",
       " 'lunch',\n",
       " 'at',\n",
       " 'home,',\n",
       " 'so',\n",
       " 'he',\n",
       " 'had',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'a',\n",
       " 'burger',\n",
       " 'on',\n",
       " 'his',\n",
       " 'way',\n",
       " 'to',\n",
       " 'work']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word2vec\n",
    "\n",
    "skip_gram = Word2Vec(sentences, vector_size=300, min_count=1, window=5, sg=1)\n",
    "cbow = Word2Vec(sentences, vector_size=100, min_count=1, window=5, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homer 의 vector representation : \n",
      "[ 4.3701060e-04  2.2055381e-03  3.3186206e-03  2.9940126e-03\n",
      " -2.6567397e-03  2.1234783e-03 -1.8821131e-03 -2.3819870e-04\n",
      "  1.6127196e-04  2.1778548e-03  1.5117687e-03  1.5151121e-03\n",
      "  3.1674108e-03  1.1945881e-04 -2.0241914e-03 -2.1457686e-03\n",
      "  2.2043383e-03 -1.7624284e-03 -9.4591797e-04  1.2793364e-03\n",
      " -7.2408590e-04 -2.0029263e-03 -7.5762207e-04  4.2041132e-04\n",
      "  7.6597626e-04  2.0379657e-03 -1.7428604e-03  1.0297104e-03\n",
      "  2.4278299e-03  6.9946179e-04  1.7967316e-03 -1.5996851e-03\n",
      "  2.0529374e-03 -2.5236835e-03  1.1533408e-03 -3.0791005e-03\n",
      " -8.4886025e-04 -3.0566400e-03 -5.1756331e-04 -1.7981926e-03\n",
      " -1.2938769e-03  3.8499379e-04  9.4194955e-04 -5.1014154e-04\n",
      " -2.6721673e-03 -1.9110956e-03  2.8245136e-04 -1.2844786e-03\n",
      " -3.1514806e-03 -2.4119446e-04  2.2119523e-03  1.9877201e-03\n",
      " -3.3284628e-03  1.0791372e-03 -2.0474067e-03 -3.0453433e-03\n",
      "  3.5906611e-05 -8.0902988e-05 -2.3292252e-03 -2.0569263e-03\n",
      " -7.9015619e-04  2.3795723e-03 -2.5085832e-03  2.5598865e-03\n",
      " -1.9220970e-04  3.9324031e-04  3.1638236e-03  1.5923828e-03\n",
      " -1.1991509e-03  1.2176029e-03  1.1792700e-03  2.1423080e-03\n",
      "  5.4903809e-05 -1.5288970e-03  4.1544464e-04 -1.7971572e-03\n",
      "  4.4534815e-04  1.6214189e-03  1.7140113e-03  3.1447702e-03\n",
      " -2.5179903e-03 -1.8049591e-03  2.1555205e-03  5.2429287e-04\n",
      " -2.2119668e-03  3.1118159e-04  8.5618004e-04 -8.1406150e-04\n",
      " -1.6071454e-03  1.6540424e-03  3.2269801e-03 -2.4326083e-03\n",
      " -1.4112348e-05 -8.4898638e-04 -2.1009580e-03 -4.2406068e-04\n",
      " -1.7140621e-03  2.9630207e-03 -1.9163737e-03  1.2541314e-03\n",
      " -8.5312102e-05  1.4319060e-03  7.3685881e-04  3.3522919e-03\n",
      "  2.3113852e-04 -1.8503263e-03 -3.6054620e-04  7.1927282e-04\n",
      " -1.2104339e-03 -2.6238812e-03 -1.8887687e-03 -2.2957046e-03\n",
      "  2.1075888e-03  1.2962973e-03  2.7266820e-03  2.1808653e-03\n",
      " -2.0522040e-03  8.9660392e-04  2.8247773e-03  4.8115861e-04\n",
      "  1.0146219e-03  1.9661933e-03 -2.9192038e-03  3.0447133e-03\n",
      "  2.2504404e-03  2.8548623e-03 -2.7574664e-03  2.0591679e-03\n",
      "  2.2178944e-03 -4.1130031e-04 -2.0891600e-03  1.7887193e-03\n",
      " -2.2667162e-03 -1.7930600e-03  1.1868610e-03  2.7135611e-03\n",
      "  2.8882101e-03 -1.4881657e-03 -3.0607569e-03  3.1996637e-03\n",
      "  2.1224043e-03 -1.3589793e-03 -2.8336656e-03 -1.5542720e-03\n",
      " -1.2908998e-03 -1.1244731e-03  2.3993685e-04 -1.0035835e-04\n",
      " -1.0106890e-03 -2.0243055e-03  3.1502771e-03 -1.6066595e-03\n",
      " -2.4102994e-03  2.5592824e-03  8.4109470e-04  2.9028964e-03\n",
      " -1.4963046e-03 -2.3331475e-03  2.9733090e-04 -3.3361960e-04\n",
      " -3.1624483e-03 -4.8376300e-04  9.9931448e-04  2.2019527e-03\n",
      "  2.2097572e-03  1.0774730e-03 -1.4738941e-03 -5.8954238e-04\n",
      " -1.3410242e-03  1.9713459e-03 -2.1535940e-03  6.7809527e-04\n",
      " -4.2884107e-04 -1.8842067e-03 -2.4058416e-03  1.9657379e-03\n",
      " -2.7225604e-03 -2.7745083e-04  9.7041216e-04  2.6040026e-03\n",
      " -2.4311852e-03  1.1376997e-03  3.2403264e-03 -2.3576764e-03\n",
      " -1.1970098e-03  1.7210388e-03  1.7466285e-03  5.9898535e-04\n",
      "  2.6580237e-03  2.2044008e-04  6.0786144e-04 -5.3200690e-04\n",
      " -2.7464088e-03  1.0881461e-03  6.6089380e-04 -2.9329748e-03\n",
      " -2.1092636e-04  5.3897315e-07  8.5134179e-06  2.8763919e-03\n",
      " -8.5464993e-04 -1.9270343e-03  2.4978903e-03 -2.4266308e-03\n",
      " -2.9650545e-03 -6.1467930e-04 -2.7719710e-03  1.2602033e-04\n",
      "  6.8324961e-04 -8.1649388e-04 -2.1624973e-03 -1.3252154e-04\n",
      " -3.8072738e-04  1.1368409e-03  2.7700840e-03  1.9352076e-03\n",
      "  2.7616913e-03 -3.0331330e-03  3.0830889e-03 -8.1125105e-04\n",
      "  2.8862080e-03  7.9261162e-04  1.1756852e-03 -3.2193374e-03\n",
      " -3.1929875e-03  2.9936947e-03 -9.2571235e-04  9.1703382e-04\n",
      "  2.1497642e-03 -1.1029757e-04  3.3070680e-03 -3.8661546e-04\n",
      " -3.2316754e-03 -2.3254044e-03 -4.2328655e-04 -2.9158047e-03\n",
      "  2.4747641e-03  1.1723287e-03 -2.9311557e-03  2.7570750e-03\n",
      "  2.9867012e-03  1.9745468e-03  2.2923278e-03 -3.1687804e-03\n",
      "  1.4838368e-04 -3.1274783e-03 -1.1956363e-03  8.1394079e-05\n",
      " -9.7301221e-05  4.7507611e-04  1.0957054e-03  7.0291857e-04\n",
      "  1.8156926e-03  2.5130273e-03 -2.0152903e-03  2.6291865e-03\n",
      "  2.0141937e-03  3.2460506e-03  1.5089754e-03 -1.1392875e-03\n",
      " -1.2257859e-03 -2.1433647e-04 -5.0778169e-04  3.2442368e-03\n",
      "  3.6563096e-04  1.3440289e-03  1.1411036e-03 -2.9123665e-03\n",
      "  2.1757190e-03  2.7458805e-03 -5.8946881e-04  7.8874396e-04\n",
      " -2.1669052e-03 -2.0417976e-03  2.7617740e-03 -9.7453612e-04\n",
      "  2.2337004e-03  5.5373117e-04 -6.8602758e-04  9.6204772e-04\n",
      " -1.3523692e-03 -5.9573748e-04  4.3957916e-04  1.6169741e-03\n",
      " -4.6849396e-04  2.7791530e-04 -2.7011384e-03  3.1968558e-03\n",
      " -2.8990330e-03  2.3458411e-03  1.2981907e-03 -2.3006673e-03\n",
      " -1.7402709e-03 -2.6324647e-03 -1.6976167e-03  2.4079145e-03\n",
      "  3.2132810e-03  7.0511841e-04  2.4746294e-04  3.1380025e-03]\n"
     ]
    }
   ],
   "source": [
    "print(\"{} 의 vector representation : \\n{}\".format('homer', skip_gram.wv.get_vector(skip_gram.wv.key_to_index['homer'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('video', 0.14035673439502716),\n",
       " ('his', 0.12365606427192688),\n",
       " ('adorable', 0.11183691769838333),\n",
       " ('burger', 0.10865367203950882),\n",
       " ('planned', 0.09786190837621689),\n",
       " ('she', 0.09258976578712463),\n",
       " ('do', 0.09069041162729263),\n",
       " ('as', 0.08781418949365616),\n",
       " ('concert', 0.08753109723329544),\n",
       " ('lisa', 0.08640199154615402)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_gram.wv.most_similar(\"homer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "homer_vector = skip_gram.wv.get_vector(skip_gram.wv.key_to_index['homer'])\n",
    "video_vector = skip_gram.wv.get_vector(skip_gram.wv.key_to_index['video'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도 계산하기 from scratch\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vector_a, vector_b):\n",
    "    \"\"\"Calculate the cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = np.dot(vector_a, vector_b)\n",
    "    norm_a = norm(vector_a)\n",
    "    norm_b = norm(vector_b)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14035673"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(homer_vector, video_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simpsons dataset을 활용한 Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.edrawmax.com/what-is/simpsons-family-tree/example.png) <br>\n",
    "출처 : https://images.edrawmax.com/what-is/simpsons-family-tree/example.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158314, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('simpsons_dataset.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        raw_character_text                                       spoken_words\n",
       "0              Miss Hoover  No, actually, it was a little of both. Sometim...\n",
       "1             Lisa Simpson                             Where's Mr. Bergstrom?\n",
       "2              Miss Hoover  I don't know. Although I'd sure like to talk t...\n",
       "3             Lisa Simpson                         That life is worth living.\n",
       "4  Edna Krabappel-Flanders  The polls will be open from now until the end ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_character_text    17814\n",
       "spoken_words          26459\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No, actually, it was a little of both. Sometimes when a disease is in all the magazines and all the news shows, it's only natural that you think you have it.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'spoken_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are lemmatizing and removing the stopwords and non-alphabetic characters for each line of dialogue.\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "\n",
    "def cleaning(doc):\n",
    "    # Lemmatizes and remove stopwords\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep alphabets\n",
    "cleaner = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['spoken_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "txt = [cleaning(doc) for doc in nlp.pipe(cleaner, batch_size=5000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'actually little disease magazine news show natural think'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85956, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe에 넣어서 null이 있는 대화는 삭제\n",
    "# 주로 null은 특정 행동을 했지만 대화가 없었을 때임\n",
    "\n",
    "df_clean = pd.DataFrame({'clean': txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 문장을 여러 단위의 단어로 분할\n",
    "sentences = [s.split(' ') for s in df_clean['clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85956"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `window` : 문장 내에서 현재 단어와 예측 단어 사이의 최대 거리. ex) 타겟 단어의 왼쪽과 오른쪽 n번째 단어\n",
    "- `vector_size` : 단어 벡터의 차원 수\n",
    "- `min_count` : 이 값보다 총 절대 빈도수가 낮은 모든 단어를 무시함 - (2, 100)\n",
    "- `sg` : 1은 skip-gram, 0은 CBOW method를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의 하기\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     vector_size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장에 들어있는 각 단어들을 Word2Vec 모델이 인식할 수 있는 형태로 변환\n",
    "w2v_model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19987756, 54001900)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(w2v_model.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어간 유사도 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(w2v_model.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- most_similar : 주어진 조건에 가장 적합한 단어 탐색\n",
    "- similarity : 주어진 단어들의 유사도 계산\n",
    "- doesnt_match : 주어진 단어들 중 가장 '덜 유사한' 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(w2v_model.wv.most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(w2v_model.wv.similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marge', 0.42559731006622314),\n",
       " ('simpson', 0.36902758479118347),\n",
       " ('bart', 0.32775843143463135),\n",
       " ('mr', 0.28152111172676086),\n",
       " ('lisa', 0.2735844552516937),\n",
       " ('wife', 0.2713277339935303),\n",
       " ('people', 0.2501447796821594),\n",
       " ('son', 0.21831999719142914),\n",
       " ('moe', 0.2168947011232376),\n",
       " ('family', 0.20857103168964386)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"homer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lisa', 0.4613502621650696),\n",
       " ('dad', 0.35596969723701477),\n",
       " ('boy', 0.3391043245792389),\n",
       " ('homer', 0.3277583718299866),\n",
       " ('child', 0.30410629510879517),\n",
       " ('mom', 0.302735298871994),\n",
       " ('milhouse', 0.29639771580696106),\n",
       " ('parent', 0.29103147983551025),\n",
       " ('mother', 0.28199145197868347),\n",
       " ('kid', 0.2726426422595978)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"bart\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 0.2712000608444214),\n",
       " ('guide', 0.2056269347667694),\n",
       " ('modern', 0.19630517065525055)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"woman\", \"homer\"], negative=[\"marge\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lisa', 0.25555509328842163),\n",
       " ('mom', 0.23903565108776093),\n",
       " ('embarrassing', 0.22747930884361267)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"woman\", \"bart\"], negative=[\"man\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bart'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match(['bart', 'homer', 'marge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'marge'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match(['bart', 'lisa', 'marge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 임베딩의 한계점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_vector = w2v_model.wv.get_vector(w2v_model.wv.key_to_index['bank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어에 불과하기 때문에 context를 고려하지 못 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.48501942,  0.43569145, -0.29942   , -0.22898197, -0.9711868 ,\n",
       "       -0.32046235,  0.5643262 , -0.01186122, -0.5335374 ,  0.27579722,\n",
       "       -0.18607752,  0.9545646 ,  0.20190725, -0.34718508,  0.6243724 ,\n",
       "       -0.06537967,  0.12549879,  0.75643426,  0.8152887 ,  0.7127831 ,\n",
       "       -0.7926212 ,  1.0434473 , -1.4697722 , -0.36695647,  1.1339116 ,\n",
       "        0.49978697,  0.92579854, -1.8430809 ,  0.05365827, -0.81561154,\n",
       "       -0.505158  ,  0.03750321,  0.17283091, -0.21995084,  0.6998929 ,\n",
       "        0.24177593,  0.98734653,  0.1569476 , -0.4521674 , -0.7442057 ,\n",
       "       -0.13561198,  0.22614641, -0.77001715, -0.88620585,  0.17705469,\n",
       "       -0.28627655,  0.43960652, -0.29148743, -0.38570356, -1.1539671 ,\n",
       "       -0.7714859 ,  0.04027198, -0.32321674, -0.22973727,  0.29156774,\n",
       "        0.6530905 ,  1.428275  ,  0.83454007,  0.38090903,  0.18053168,\n",
       "        0.35904467,  0.19431151,  0.3605693 ,  0.19162191,  0.2537165 ,\n",
       "       -1.6577637 , -0.45984957,  0.29928204, -0.70044196, -0.04974677,\n",
       "       -0.42815566, -0.59092057, -0.35566646,  0.8821294 , -0.0280409 ,\n",
       "       -0.46728298,  0.4361739 ,  0.26156676, -0.11868543,  0.57859606,\n",
       "       -0.27648038, -0.10592568, -0.05297432,  0.66885257,  0.0776957 ,\n",
       "        0.37689835,  0.9896285 , -0.5357174 , -0.6113083 , -0.6154293 ,\n",
       "        0.46004558,  0.35907245, -0.32930386,  0.81214476,  0.43520486,\n",
       "        0.6731961 ,  0.28269032, -0.732067  ,  0.59106284, -0.14472961,\n",
       "        0.12962294, -0.9697643 ,  0.62850195,  0.41403562, -0.23389342,\n",
       "       -0.19467393, -0.81770647, -0.74711055,  0.40464818,  0.1727456 ,\n",
       "       -0.02631837, -1.0872741 ,  0.23592973,  0.19576102,  0.20499496,\n",
       "       -0.3996157 ,  0.19922595,  0.0661834 , -0.44908386, -0.17273575,\n",
       "       -0.60175127, -0.18059863,  0.09798596,  0.36140785, -0.10896903,\n",
       "        0.3768986 ,  0.50025874,  0.0421824 , -0.25775602, -1.2823884 ,\n",
       "        0.06285959, -0.60963744, -0.09503266,  0.19136317, -0.40657324,\n",
       "        0.32407823,  0.35384208, -0.0100591 , -1.1121477 , -1.0643346 ,\n",
       "        0.11591943, -0.14615159,  1.1744244 ,  1.166323  , -0.4000272 ,\n",
       "       -0.58014274, -0.46825624,  0.8919087 ,  0.07487456, -0.03384471,\n",
       "        0.60870326, -1.407722  ,  0.5116874 , -0.5499926 ,  0.08728237,\n",
       "       -0.2583805 ,  0.2032777 ,  0.07042511,  0.22952008, -0.301142  ,\n",
       "        1.0110656 , -0.06147619,  0.55330193, -0.34325162, -0.46376133,\n",
       "       -1.2429619 ,  0.34319547,  0.91365194, -1.080102  ,  0.06352733,\n",
       "       -0.99455714,  1.1942861 , -0.16994295,  0.45534614,  0.811821  ,\n",
       "       -0.7372436 ,  0.632122  ,  0.12997411, -0.75181085, -0.35153374,\n",
       "       -0.9210219 , -0.73816216,  0.58731204,  1.0127479 ,  0.50572103,\n",
       "        0.7367457 ,  0.82665896,  0.79502493,  0.8514853 ,  0.25518647,\n",
       "        0.45350608,  0.52537984,  0.76080173,  0.0342546 ,  0.42801258,\n",
       "       -0.43260723, -0.33127317, -0.6650551 , -0.16336992, -0.23611526,\n",
       "       -0.85600764, -0.2531454 , -0.94909847, -0.12261002,  0.09248812,\n",
       "       -0.02476502, -0.48354274,  0.62543803,  1.136964  ,  1.0882865 ,\n",
       "        0.2625769 ,  0.4387636 ,  0.37772635, -0.64433885,  0.19162506,\n",
       "       -0.5617094 , -0.01280659,  0.7930472 ,  1.3626887 ,  0.1917445 ,\n",
       "        0.9007646 , -0.31355423, -0.264138  ,  0.5959666 , -0.66682845,\n",
       "        1.1258081 ,  1.711072  , -0.385125  ,  0.2231059 ,  0.34353405,\n",
       "       -0.7388666 , -1.5970043 , -0.08997786, -0.68921417, -0.1910654 ,\n",
       "        0.06576824, -0.504735  , -0.74830025,  1.534628  , -0.59861326,\n",
       "       -0.18551701,  0.37420404, -0.42178962,  0.6967116 ,  1.2953206 ,\n",
       "       -0.3160671 , -0.4421376 , -0.02398746,  0.1485184 ,  0.8239136 ,\n",
       "       -1.3177432 ,  1.345344  , -1.0390495 ,  0.18846965, -0.22514097,\n",
       "       -0.04176788,  0.22704203,  1.1070007 ,  0.3631048 ,  0.43626735,\n",
       "       -0.03271411, -0.59105074,  0.3894586 , -0.48798376,  1.6623874 ,\n",
       "        0.629672  , -0.7367748 , -0.2920217 , -0.30006212, -0.9059607 ,\n",
       "       -0.30829662, -1.0300587 , -0.36454624,  0.20503719,  0.78351116,\n",
       "       -0.16882758,  0.11967155, -0.20894097, -1.0686772 , -0.00706962,\n",
       "        0.5928013 ,  0.48877493,  0.48600486, -0.38151717,  0.5606441 ,\n",
       "        0.2639638 , -0.06925626,  0.52608675, -0.6895902 , -0.51725304,\n",
       "        0.23623192,  0.10885553,  0.40332577, -0.03270952, -0.18533157,\n",
       "       -1.0064334 ,  0.49408257, -0.43889943, -0.14379591, -0.93715125],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gieunkwak/opt/anaconda3/envs/fastcampus/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 3.70MB/s]\n"
     ]
    }
   ],
   "source": [
    "# pre-trained model tokenizer와 and bert model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # smaller & uncased model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bank가 들어간 유사한 문장 두 개\n",
    "sentence1 = \"I deposited money at the bank.\"\n",
    "sentence2 = \"The ducks swam to the river bank.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 BERT가 인식할 수 있는 형태로 Tokenize\n",
    "encoded_input1 = tokenizer(sentence1, return_tensors='pt')\n",
    "encoded_input2 = tokenizer(sentence2, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045, 14140,  2769,  2012,  1996,  2924,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `input_ids` : 각 단어별로 매핑된 key. 101은 문장의 시작을, 102는 문장의 끝을 의미\n",
    "- `token_type_ids` : 문장 번호\n",
    "- `attention_mask` : attention을 가져야 하는 단어는 1, 그렇지 않은 단어는 0. (만약 input이 실제 단어들이라면 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding 생성!\n",
    "with torch.no_grad():\n",
    "    output1 = model(**encoded_input1)\n",
    "    output2 = model(**encoded_input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding 내에서 bank라는 단어 찾아오기 (문장의 5번째에 있는 단어)\n",
    "bank_embedding_sentence1 = output1.last_hidden_state[0, 5, :]\n",
    "bank_embedding_sentence2 = output2.last_hidden_state[0, 5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between the two embeddings: tensor(0.5922)\n"
     ]
    }
   ],
   "source": [
    "# cosine similarity 계산을 통해 얼마나 유사한지 검증\n",
    "\n",
    "similarity = torch.nn.functional.cosine_similarity(bank_embedding_sentence1, bank_embedding_sentence2, dim=0)\n",
    "\n",
    "# print(\"Embedding for 'bank' in sentence 1:\", bank_embedding_sentence1)\n",
    "# print(\"Embedding for 'bank' in sentence 2:\", bank_embedding_sentence2)\n",
    "print(\"Cosine similarity between the two embeddings:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
