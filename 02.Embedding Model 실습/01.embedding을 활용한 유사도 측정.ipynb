{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/gieunkwak/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gieunkwak/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec을 이용한 단어 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT야, simpsons 캐릭터 이름이 들어간 랜덤 문장 10개를 생성해줘\n",
    "\n",
    "sentences = [\"Homer Simpson forgot his lunch at home, so he had to buy a burger on his way to work.\",\n",
    "    \"Marge was busy knitting a new sweater for Bart's upcoming school play.\",\n",
    "    \"Lisa Simpson played a beautiful saxophone solo at the school concert.\",\n",
    "    \"Mr. Burns secretly plotted another scheme from his office at the Springfield Nuclear Power Plant.\",\n",
    "    \"Ned Flanders offered to help Homer fix the fence between their houses.\",\n",
    "    \"Bart Simpson tried a new prank at school, but it didn't go as planned.\",\n",
    "    \"Milhouse and Bart spent the afternoon playing video games and forgot to do their homework.\",\n",
    "    \"Maggie Simpson's adorable giggle filled the room as she played with her toys.\",\n",
    "    \"Apu had a busy day at the Kwik-E-Mart, dealing with a rush of customers.\",\n",
    "    \"Krusty the Clown decided to change his show a bit to attract a new audience.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "# get rid of stopwords, lower case\n",
    "\n",
    "sentences = [s.lower().replace(\".\", \"\").split(\" \") for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['homer',\n",
       " 'simpson',\n",
       " 'forgot',\n",
       " 'his',\n",
       " 'lunch',\n",
       " 'at',\n",
       " 'home,',\n",
       " 'so',\n",
       " 'he',\n",
       " 'had',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'a',\n",
       " 'burger',\n",
       " 'on',\n",
       " 'his',\n",
       " 'way',\n",
       " 'to',\n",
       " 'work']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word2vec\n",
    "\n",
    "skip_gram = Word2Vec(sentences, vector_size=300, min_count=1, window=5, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homer 의 vector representation : \n",
      "[ 4.3701060e-04  2.2055381e-03  3.3186206e-03  2.9940126e-03\n",
      " -2.6567397e-03  2.1234783e-03 -1.8821131e-03 -2.3819870e-04\n",
      "  1.6127196e-04  2.1778548e-03  1.5117687e-03  1.5151121e-03\n",
      "  3.1674108e-03  1.1945881e-04 -2.0241914e-03 -2.1457686e-03\n",
      "  2.2043383e-03 -1.7624284e-03 -9.4591797e-04  1.2793364e-03\n",
      " -7.2408590e-04 -2.0029263e-03 -7.5762207e-04  4.2041132e-04\n",
      "  7.6597626e-04  2.0379657e-03 -1.7428604e-03  1.0297104e-03\n",
      "  2.4278299e-03  6.9946179e-04  1.7967316e-03 -1.5996851e-03\n",
      "  2.0529374e-03 -2.5236835e-03  1.1533408e-03 -3.0791005e-03\n",
      " -8.4886025e-04 -3.0566400e-03 -5.1756331e-04 -1.7981926e-03\n",
      " -1.2938769e-03  3.8499379e-04  9.4194955e-04 -5.1014154e-04\n",
      " -2.6721673e-03 -1.9110956e-03  2.8245136e-04 -1.2844786e-03\n",
      " -3.1514806e-03 -2.4119446e-04  2.2119523e-03  1.9877201e-03\n",
      " -3.3284628e-03  1.0791372e-03 -2.0474067e-03 -3.0453433e-03\n",
      "  3.5906611e-05 -8.0902988e-05 -2.3292252e-03 -2.0569263e-03\n",
      " -7.9015619e-04  2.3795723e-03 -2.5085832e-03  2.5598865e-03\n",
      " -1.9220970e-04  3.9324031e-04  3.1638236e-03  1.5923828e-03\n",
      " -1.1991509e-03  1.2176029e-03  1.1792700e-03  2.1423080e-03\n",
      "  5.4903809e-05 -1.5288970e-03  4.1544464e-04 -1.7971572e-03\n",
      "  4.4534815e-04  1.6214189e-03  1.7140113e-03  3.1447702e-03\n",
      " -2.5179903e-03 -1.8049591e-03  2.1555205e-03  5.2429287e-04\n",
      " -2.2119668e-03  3.1118159e-04  8.5618004e-04 -8.1406150e-04\n",
      " -1.6071454e-03  1.6540424e-03  3.2269801e-03 -2.4326083e-03\n",
      " -1.4112348e-05 -8.4898638e-04 -2.1009580e-03 -4.2406068e-04\n",
      " -1.7140621e-03  2.9630207e-03 -1.9163737e-03  1.2541314e-03\n",
      " -8.5312102e-05  1.4319060e-03  7.3685881e-04  3.3522919e-03\n",
      "  2.3113852e-04 -1.8503263e-03 -3.6054620e-04  7.1927282e-04\n",
      " -1.2104339e-03 -2.6238812e-03 -1.8887687e-03 -2.2957046e-03\n",
      "  2.1075888e-03  1.2962973e-03  2.7266820e-03  2.1808653e-03\n",
      " -2.0522040e-03  8.9660392e-04  2.8247773e-03  4.8115861e-04\n",
      "  1.0146219e-03  1.9661933e-03 -2.9192038e-03  3.0447133e-03\n",
      "  2.2504404e-03  2.8548623e-03 -2.7574664e-03  2.0591679e-03\n",
      "  2.2178944e-03 -4.1130031e-04 -2.0891600e-03  1.7887193e-03\n",
      " -2.2667162e-03 -1.7930600e-03  1.1868610e-03  2.7135611e-03\n",
      "  2.8882101e-03 -1.4881657e-03 -3.0607569e-03  3.1996637e-03\n",
      "  2.1224043e-03 -1.3589793e-03 -2.8336656e-03 -1.5542720e-03\n",
      " -1.2908998e-03 -1.1244731e-03  2.3993685e-04 -1.0035835e-04\n",
      " -1.0106890e-03 -2.0243055e-03  3.1502771e-03 -1.6066595e-03\n",
      " -2.4102994e-03  2.5592824e-03  8.4109470e-04  2.9028964e-03\n",
      " -1.4963046e-03 -2.3331475e-03  2.9733090e-04 -3.3361960e-04\n",
      " -3.1624483e-03 -4.8376300e-04  9.9931448e-04  2.2019527e-03\n",
      "  2.2097572e-03  1.0774730e-03 -1.4738941e-03 -5.8954238e-04\n",
      " -1.3410242e-03  1.9713459e-03 -2.1535940e-03  6.7809527e-04\n",
      " -4.2884107e-04 -1.8842067e-03 -2.4058416e-03  1.9657379e-03\n",
      " -2.7225604e-03 -2.7745083e-04  9.7041216e-04  2.6040026e-03\n",
      " -2.4311852e-03  1.1376997e-03  3.2403264e-03 -2.3576764e-03\n",
      " -1.1970098e-03  1.7210388e-03  1.7466285e-03  5.9898535e-04\n",
      "  2.6580237e-03  2.2044008e-04  6.0786144e-04 -5.3200690e-04\n",
      " -2.7464088e-03  1.0881461e-03  6.6089380e-04 -2.9329748e-03\n",
      " -2.1092636e-04  5.3897315e-07  8.5134179e-06  2.8763919e-03\n",
      " -8.5464993e-04 -1.9270343e-03  2.4978903e-03 -2.4266308e-03\n",
      " -2.9650545e-03 -6.1467930e-04 -2.7719710e-03  1.2602033e-04\n",
      "  6.8324961e-04 -8.1649388e-04 -2.1624973e-03 -1.3252154e-04\n",
      " -3.8072738e-04  1.1368409e-03  2.7700840e-03  1.9352076e-03\n",
      "  2.7616913e-03 -3.0331330e-03  3.0830889e-03 -8.1125105e-04\n",
      "  2.8862080e-03  7.9261162e-04  1.1756852e-03 -3.2193374e-03\n",
      " -3.1929875e-03  2.9936947e-03 -9.2571235e-04  9.1703382e-04\n",
      "  2.1497642e-03 -1.1029757e-04  3.3070680e-03 -3.8661546e-04\n",
      " -3.2316754e-03 -2.3254044e-03 -4.2328655e-04 -2.9158047e-03\n",
      "  2.4747641e-03  1.1723287e-03 -2.9311557e-03  2.7570750e-03\n",
      "  2.9867012e-03  1.9745468e-03  2.2923278e-03 -3.1687804e-03\n",
      "  1.4838368e-04 -3.1274783e-03 -1.1956363e-03  8.1394079e-05\n",
      " -9.7301221e-05  4.7507611e-04  1.0957054e-03  7.0291857e-04\n",
      "  1.8156926e-03  2.5130273e-03 -2.0152903e-03  2.6291865e-03\n",
      "  2.0141937e-03  3.2460506e-03  1.5089754e-03 -1.1392875e-03\n",
      " -1.2257859e-03 -2.1433647e-04 -5.0778169e-04  3.2442368e-03\n",
      "  3.6563096e-04  1.3440289e-03  1.1411036e-03 -2.9123665e-03\n",
      "  2.1757190e-03  2.7458805e-03 -5.8946881e-04  7.8874396e-04\n",
      " -2.1669052e-03 -2.0417976e-03  2.7617740e-03 -9.7453612e-04\n",
      "  2.2337004e-03  5.5373117e-04 -6.8602758e-04  9.6204772e-04\n",
      " -1.3523692e-03 -5.9573748e-04  4.3957916e-04  1.6169741e-03\n",
      " -4.6849396e-04  2.7791530e-04 -2.7011384e-03  3.1968558e-03\n",
      " -2.8990330e-03  2.3458411e-03  1.2981907e-03 -2.3006673e-03\n",
      " -1.7402709e-03 -2.6324647e-03 -1.6976167e-03  2.4079145e-03\n",
      "  3.2132810e-03  7.0511841e-04  2.4746294e-04  3.1380025e-03]\n"
     ]
    }
   ],
   "source": [
    "print(\"{} 의 vector representation : \\n{}\".format('homer', skip_gram.wv.get_vector(skip_gram.wv.key_to_index['homer'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('video', 0.14035673439502716),\n",
       " ('his', 0.12365606427192688),\n",
       " ('adorable', 0.11183691769838333),\n",
       " ('burger', 0.10865367203950882),\n",
       " ('planned', 0.09786190837621689),\n",
       " ('she', 0.09258976578712463),\n",
       " ('do', 0.09069041162729263),\n",
       " ('as', 0.08781418949365616),\n",
       " ('concert', 0.08753109723329544),\n",
       " ('lisa', 0.08640199154615402)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_gram.wv.most_similar(\"homer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직접 유사도 구해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "homer_vector = skip_gram.wv.get_vector(skip_gram.wv.key_to_index['homer'])\n",
    "video_vector = skip_gram.wv.get_vector(skip_gram.wv.key_to_index['video'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도 계산하기 from scratch\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vector_a, vector_b):\n",
    "    \"\"\"Calculate the cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = np.dot(vector_a, vector_b)\n",
    "    norm_a = norm(vector_a)\n",
    "    norm_b = norm(vector_b)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14035673"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(homer_vector, video_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simpsons dataset을 활용한 Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.edrawmax.com/what-is/simpsons-family-tree/example.png) <br>\n",
    "출처 : https://images.edrawmax.com/what-is/simpsons-family-tree/example.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158314, 2)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('simpsons_dataset.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        raw_character_text                                       spoken_words\n",
       "0              Miss Hoover  No, actually, it was a little of both. Sometim...\n",
       "1             Lisa Simpson                             Where's Mr. Bergstrom?\n",
       "2              Miss Hoover  I don't know. Although I'd sure like to talk t...\n",
       "3             Lisa Simpson                         That life is worth living.\n",
       "4  Edna Krabappel-Flanders  The polls will be open from now until the end ..."
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_character_text    17814\n",
       "spoken_words          26459\n",
       "dtype: int64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No, actually, it was a little of both. Sometimes when a disease is in all the magazines and all the news shows, it's only natural that you think you have it.\""
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'spoken_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are lemmatizing and removing the stopwords and non-alphabetic characters for each line of dialogue.\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "\n",
    "def cleaning(doc):\n",
    "    # Lemmatizes and remove stopwords\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep alphabets\n",
    "cleaner = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['spoken_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = [cleaning(doc) for doc in nlp.pipe(cleaner, batch_size=5000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'actually little disease magazine news show natural think'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85956, 1)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe에 넣어서 null이 있는 대화는 삭제\n",
    "# 주로 null은 특정 행동을 했지만 대화가 없었을 때임\n",
    "\n",
    "df_clean = pd.DataFrame({'clean': txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 문장을 여러 단위의 단어로 분할\n",
    "sentences = [s.split(' ') for s in df_clean['clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85956"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actually',\n",
       " 'little',\n",
       " 'disease',\n",
       " 'magazine',\n",
       " 'news',\n",
       " 'show',\n",
       " 'natural',\n",
       " 'think']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Word2Vec in module gensim.models.word2vec:\n",
      "\n",
      "class Word2Vec(gensim.utils.SaveLoad)\n",
      " |  Word2Vec(sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None, shrink_windows=True)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Word2Vec\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None, shrink_windows=True)\n",
      " |      Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
      " |      \n",
      " |      Once you're finished training a model (=no more updates, only querying)\n",
      " |      store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in ``self.wv``\n",
      " |      to reduce memory.\n",
      " |      \n",
      " |      The full model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
      " |      :meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
      " |      \n",
      " |      The trained word vectors can also be stored/loaded from a format compatible with the\n",
      " |      original word2vec implementation via `self.wv.save_word2vec_format`\n",
      " |      and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of iterables, optional\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |          If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
      " |          in some other way.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
      " |      vector_size : int, optional\n",
      " |          Dimensionality of the word vectors.\n",
      " |      window : int, optional\n",
      " |          Maximum distance between the current and predicted word within a sentence.\n",
      " |      min_count : int, optional\n",
      " |          Ignores all words with total frequency lower than this.\n",
      " |      workers : int, optional\n",
      " |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      " |      sg : {0, 1}, optional\n",
      " |          Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
      " |      hs : {0, 1}, optional\n",
      " |          If 1, hierarchical softmax will be used for model training.\n",
      " |          If 0, and `negative` is non-zero, negative sampling will be used.\n",
      " |      negative : int, optional\n",
      " |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      " |          should be drawn (usually between 5-20).\n",
      " |          If set to 0, no negative sampling is used.\n",
      " |      ns_exponent : float, optional\n",
      " |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      " |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      " |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      " |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
      " |          other values may perform better for recommendation applications.\n",
      " |      cbow_mean : {0, 1}, optional\n",
      " |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
      " |      alpha : float, optional\n",
      " |          The initial learning rate.\n",
      " |      min_alpha : float, optional\n",
      " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      " |      seed : int, optional\n",
      " |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      " |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      " |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      " |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
      " |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
      " |      max_vocab_size : int, optional\n",
      " |          Limits the RAM during vocabulary building; if there are more unique\n",
      " |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      " |          Set to `None` for no limit.\n",
      " |      max_final_vocab : int, optional\n",
      " |          Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
      " |          min_count is more than the calculated min_count, the specified min_count will be used.\n",
      " |          Set to `None` if not required.\n",
      " |      sample : float, optional\n",
      " |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      " |          useful range is (0, 1e-5).\n",
      " |      hashfxn : function, optional\n",
      " |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
      " |      epochs : int, optional\n",
      " |          Number of iterations (epochs) over the corpus. (Formerly: `iter`)\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
      " |          model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      sorted_vocab : {0, 1}, optional\n",
      " |          If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
      " |          See :meth:`~gensim.models.keyedvectors.KeyedVectors.sort_by_descending_frequency()`.\n",
      " |      batch_words : int, optional\n",
      " |          Target size (in words) for batches of examples passed to worker threads (and\n",
      " |          thus cython routines).(Larger batches will be passed if individual\n",
      " |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |      shrink_windows : bool, optional\n",
      " |          New in 4.1. Experimental.\n",
      " |          If True, the effective window size is uniformly sampled from  [1, `window`]\n",
      " |          for each target word during training, to match the original word2vec algorithm's\n",
      " |          approximate weighting of context words by distance. Otherwise, the effective\n",
      " |          window size is always fixed to `window` words to either side.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>> model = Word2Vec(sentences, min_count=1)\n",
      " |      \n",
      " |      Attributes\n",
      " |      ----------\n",
      " |      wv : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
      " |          directly to query those embeddings in various ways. See the module level docstring for examples.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Human readable representation of the model's state.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Human readable representation of the model's state, including the vocabulary size, vector size\n",
      " |          and learning rate.\n",
      " |  \n",
      " |  add_null_word(self)\n",
      " |  \n",
      " |  build_vocab(self, corpus_iterable=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      " |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus_iterable : iterable of list of str\n",
      " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` module for such examples.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      update : bool\n",
      " |          If true, the new words in `sentences` will be added to model's vocab.\n",
      " |      progress_per : int, optional\n",
      " |          Indicates how many words to process before showing/updating the progress.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, the raw vocabulary will be deleted after the scaling is done to free up RAM.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      **kwargs : object\n",
      " |          Keyword arguments propagated to `self.prepare_vocab`.\n",
      " |  \n",
      " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      " |      Build vocabulary from a dictionary of word frequencies.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_freq : dict of (str, int)\n",
      " |          A mapping from a word in the vocabulary to its frequency count.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n",
      " |      corpus_count : int, optional\n",
      " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      update : bool, optional\n",
      " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      " |  \n",
      " |  create_binary_tree(self)\n",
      " |      Create a `binary Huffman tree <https://en.wikipedia.org/wiki/Huffman_coding>`_ using stored vocabulary\n",
      " |      word counts. Frequent words will have shorter binary codes.\n",
      " |      Called internally from :meth:`~gensim.models.word2vec.Word2VecVocab.build_vocab`.\n",
      " |  \n",
      " |  estimate_memory(self, vocab_size=None, report=None)\n",
      " |      Estimate required memory for a model using current settings and provided vocabulary size.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vocab_size : int, optional\n",
      " |          Number of unique tokens in the vocabulary\n",
      " |      report : dict of (str, int), optional\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict of (str, int)\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |  \n",
      " |  get_latest_training_loss(self)\n",
      " |      Get current value of the training loss.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Current training loss.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors. Obsoleted.\n",
      " |      \n",
      " |      If you need a single unit-normalized vector for some key, call\n",
      " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.get_vector` instead:\n",
      " |      ``word2vec_model.wv.get_vector(key, norm=True)``.\n",
      " |      \n",
      " |      To refresh norms after you performed some atypical out-of-band vector tampering,\n",
      " |      call `:meth:`~gensim.models.keyedvectors.KeyedVectors.fill_norms()` instead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool\n",
      " |          If True, forget the original trained vectors and only keep the normalized ones.\n",
      " |          You lose information if you do this.\n",
      " |  \n",
      " |  init_weights(self)\n",
      " |      Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.\n",
      " |  \n",
      " |  make_cum_table(self, domain=2147483647)\n",
      " |      Create a cumulative-distribution table using stored vocabulary word counts for\n",
      " |      drawing random words in the negative-sampling training routines.\n",
      " |      \n",
      " |      To draw a word index, choose a random integer up to the maximum value in the table (cum_table[-1]),\n",
      " |      then finding that integer's sorted insertion point (as if by `bisect_left` or `ndarray.searchsorted()`).\n",
      " |      That insertion point is the drawn index, coming up in proportion equal to the increment at that slot.\n",
      " |  \n",
      " |  predict_output_word(self, context_words_list, topn=10)\n",
      " |      Get the probability distribution of the center word given context words.\n",
      " |      \n",
      " |      Note this performs a CBOW-style propagation, even in SG models,\n",
      " |      and doesn't quite weight the surrounding words the same as in\n",
      " |      training -- so it's just one crude way of using a trained model\n",
      " |      as a predictor.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      context_words_list : list of (str and/or int)\n",
      " |          List of context words, which may be words themselves (str)\n",
      " |          or their index in `self.wv.vectors` (int).\n",
      " |      topn : int, optional\n",
      " |          Return `topn` words and their probabilities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float)\n",
      " |          `topn` length list of tuples of (word, probability).\n",
      " |  \n",
      " |  prepare_vocab(self, update=False, keep_raw_vocab=False, trim_rule=None, min_count=None, sample=None, dry_run=False)\n",
      " |      Apply vocabulary settings for `min_count` (discarding less-frequent words)\n",
      " |      and `sample` (controlling the downsampling of more-frequent words).\n",
      " |      \n",
      " |      Calling with `dry_run=True` will only simulate the provided settings and\n",
      " |      report the size of the retained vocabulary, effective corpus length, and\n",
      " |      estimated memory requirements. Results are both printed via logging and\n",
      " |      returned as a dict.\n",
      " |      \n",
      " |      Delete the raw vocabulary after the scaling is done to free up RAM,\n",
      " |      unless `keep_raw_vocab` is set.\n",
      " |  \n",
      " |  prepare_weights(self, update=False)\n",
      " |      Build tables and model weights based on final vocabulary settings.\n",
      " |  \n",
      " |  reset_from(self, other_model)\n",
      " |      Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n",
      " |      \n",
      " |      Structures copied are:\n",
      " |          * Vocabulary\n",
      " |          * Index to word mapping\n",
      " |          * Cumulative frequency table (used for negative sampling)\n",
      " |          * Cached corpus length\n",
      " |      \n",
      " |      Useful when testing multiple models on the same corpus in parallel. However, as the models\n",
      " |      then share all vocabulary-related structures other than vectors, neither should then\n",
      " |      expand their vocabulary (which could leave the other in an inconsistent, broken state).\n",
      " |      And, any changes to any per-word 'vecattr' will affect both models.\n",
      " |      \n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other_model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Another model to copy the internal structures from.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save the model.\n",
      " |      This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\n",
      " |      online training and getting vectors for vocabulary words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the file.\n",
      " |  \n",
      " |  scan_vocab(self, corpus_iterable=None, corpus_file=None, progress_per=10000, workers=None, trim_rule=None)\n",
      " |  \n",
      " |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
      " |      Score the log probability for a sequence of sentences.\n",
      " |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n",
      " |      \n",
      " |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n",
      " |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n",
      " |      \n",
      " |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n",
      " |      score more than this number of sentences but it is inefficient to set the value too high.\n",
      " |      \n",
      " |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n",
      " |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n",
      " |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n",
      " |      how to use such scores in document classification.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |      total_sentences : int, optional\n",
      " |          Count of sentences.\n",
      " |      chunksize : int, optional\n",
      " |          Chunksize of jobs\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |  \n",
      " |  seeded_vector(self, seed_string, vector_size)\n",
      " |  \n",
      " |  train(self, corpus_iterable=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=(), **kwargs)\n",
      " |      Update the model's neural weights from a sequence of sentences.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      " |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
      " |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
      " |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
      " |      you can simply use `total_examples=self.corpus_count`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      " |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      " |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.epochs`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus_iterable : iterable of list of str\n",
      " |          The ``corpus_iterable`` can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network, to limit RAM usage.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      total_examples : int\n",
      " |          Count of sentences.\n",
      " |      total_words : int\n",
      " |          Count of raw words in sentences.\n",
      " |      epochs : int\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      start_alpha : float, optional\n",
      " |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      " |          for this one call to`train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      end_alpha : float, optional\n",
      " |          Final learning rate. Drops linearly from `start_alpha`.\n",
      " |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      word_count : int, optional\n",
      " |          Count of words already trained. Set this to 0 for the usual\n",
      " |          case of training on all words in sentences.\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>>\n",
      " |          >>> model = Word2Vec(min_count=1)\n",
      " |          >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
      " |          >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)  # train word vectors\n",
      " |          (1, 30)\n",
      " |  \n",
      " |  update_weights(self)\n",
      " |      Copy all the existing weights, and reset the weights for the newly added vocabulary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, rethrow=False, **kwargs) from builtins.type\n",
      " |      Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.word2vec.Word2Vec.save`\n",
      " |          Save model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      " |      optionally log the event at `log_level`.\n",
      " |      \n",
      " |      Events are important moments during the object's life, such as \"model created\",\n",
      " |      \"model saved\", \"model loaded\", etc.\n",
      " |      \n",
      " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      " |      but is useful during debugging and support.\n",
      " |      \n",
      " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      " |      will not record events into `self.lifecycle_events` then.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      event_name : str\n",
      " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      " |      event : dict\n",
      " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      " |          Can be empty.\n",
      " |      \n",
      " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      " |      \n",
      " |          - `datetime`: the current date & time\n",
      " |          - `gensim`: the current Gensim version\n",
      " |          - `python`: the current Python version\n",
      " |          - `platform`: the current platform\n",
      " |          - `event`: the name of this event\n",
      " |      log_level : int\n",
      " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `window` : 문장 내에서 현재 단어와 예측 단어 사이의 최대 거리. ex) 타겟 단어의 왼쪽과 오른쪽 n번째 단어\n",
    "- `vector_size` : 단어 벡터의 차원 수\n",
    "- `min_count` : 이 값보다 총 절대 빈도수가 낮은 모든 단어를 무시함 - (2, 100)\n",
    "- `sg` : 1은 skip-gram, 0은 CBOW method를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의 하기\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     vector_size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장에 들어있는 각 단어들을 Word2Vec 모델이 인식할 수 있는 형태로 변환\n",
    "w2v_model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19987519, 54001900)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(w2v_model.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어간 유사도 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(w2v_model.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- most_similar : 주어진 조건에 가장 적합한 단어 탐색\n",
    "- similarity : 주어진 단어들의 유사도 계산\n",
    "- doesnt_match : 주어진 단어들 중 가장 '덜 유사한' 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(w2v_model.wv.most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(w2v_model.wv.similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marge', 0.4796435534954071),\n",
       " ('simpson', 0.3079630732536316),\n",
       " ('bart', 0.2804730534553528),\n",
       " ('right', 0.2630842626094818),\n",
       " ('husband', 0.25762954354286194),\n",
       " ('family', 0.24938347935676575),\n",
       " ('lisa', 0.2485237717628479),\n",
       " ('dad', 0.24219074845314026),\n",
       " ('son', 0.23068201541900635),\n",
       " ('mr', 0.22777511179447174)]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"homer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lisa', 0.48629072308540344),\n",
       " ('mom', 0.35374078154563904),\n",
       " ('boy', 0.31837302446365356),\n",
       " ('maggie', 0.31023937463760376),\n",
       " ('dad', 0.28740808367729187),\n",
       " ('homer', 0.2804730534553528),\n",
       " ('think', 0.2786455452442169),\n",
       " ('milhouse', 0.2761225700378418),\n",
       " ('kid', 0.2718731462955475),\n",
       " ('son', 0.2697523832321167)]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"bart\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 0.2866703271865845),\n",
       " ('people', 0.2453947514295578),\n",
       " ('see', 0.24507500231266022)]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"woman\", \"homer\"], negative=[\"marge\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('maggie', 0.3170935809612274),\n",
       " ('mom', 0.30980563163757324),\n",
       " ('lisa', 0.28484153747558594)]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"woman\", \"bart\"], negative=[\"man\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bart'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match(['bart', 'homer', 'marge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'marge'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match(['bart', 'lisa', 'marge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 임베딩의 한계점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_vector = w2v_model.wv.get_vector(w2v_model.wv.key_to_index['bank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어에 불과하기 때문에 context를 고려하지 못 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.25585392,  0.57371175, -0.06495668, -0.32777452, -1.042156  ,\n",
       "       -0.50617653,  0.8069787 , -1.4158752 , -0.13325042,  0.17344427,\n",
       "        0.10152273,  0.561469  ,  0.3041495 , -0.5154144 ,  0.07645763,\n",
       "        0.12344465, -0.24147335,  0.8387596 ,  0.49487388,  0.8492102 ,\n",
       "       -0.76920295,  0.04090631, -1.1712912 , -0.40074012,  0.7952797 ,\n",
       "        0.18193524,  0.7381184 , -1.3339674 ,  0.33916882, -0.57496434,\n",
       "       -0.6298385 , -0.15693398,  0.47853088,  0.28535807,  0.5132711 ,\n",
       "        0.44600114,  0.5698779 ,  0.269208  ,  0.01424233,  0.01953911,\n",
       "        0.5060776 ,  0.12575753, -0.5066727 , -0.9902081 ,  0.525276  ,\n",
       "        0.75072914, -0.2592828 ,  0.12266743,  0.04692424, -1.21233   ,\n",
       "       -0.5392707 , -0.10735539, -0.50581706, -0.5784349 ,  0.1565667 ,\n",
       "        0.45160735,  0.8133438 ,  1.1783154 ,  0.09559247, -0.50142986,\n",
       "        0.43354177,  0.47021854,  0.19048597, -0.5804716 ,  0.08083846,\n",
       "       -1.5428402 , -0.37920302, -0.32238775, -0.45251673, -0.1477226 ,\n",
       "       -0.29888737, -0.8142528 , -0.59726286,  0.10654464, -0.46775612,\n",
       "       -0.22221483,  0.16129893,  0.05393472,  0.17061406,  0.59567297,\n",
       "        0.06360112, -0.183018  , -0.07370333,  0.6412488 , -0.18823956,\n",
       "        0.05295453,  0.52096987, -0.9375165 ,  0.4514172 , -0.12122685,\n",
       "       -0.11278694,  0.43405968, -0.22693458,  0.43301314,  0.44230598,\n",
       "        0.6345344 ,  0.04654315, -0.51225483,  0.49862915,  0.16937542,\n",
       "        0.62505174, -0.72877365,  0.43705282,  0.13084905, -0.1801168 ,\n",
       "       -0.21615312, -0.3565996 , -0.06358427,  0.770504  ,  0.50708675,\n",
       "       -0.08484673, -0.9200365 ,  0.46401995, -0.01432358,  0.11240807,\n",
       "        0.3378006 , -0.22094645, -0.12493688, -0.6272009 ,  0.1095966 ,\n",
       "       -0.23571119,  0.704351  , -0.01484208,  0.5011745 ,  0.78644043,\n",
       "        0.1997049 ,  0.09737758, -0.54609627, -0.14234038, -1.2148167 ,\n",
       "        0.0029037 , -0.12729323,  0.7469588 ,  0.1546021 ,  0.33624548,\n",
       "       -0.10338592,  0.40542233,  0.08509567, -1.0429068 , -0.8591019 ,\n",
       "        0.28937   ,  0.28814268,  0.5883192 ,  0.3458816 ,  0.2993214 ,\n",
       "       -0.3396559 , -0.04705374,  0.58400637,  0.4034002 ,  0.36449492,\n",
       "       -0.373511  , -0.8972629 ,  0.5818521 ,  0.16156034,  0.25820163,\n",
       "        0.24853753, -0.16947684,  0.4503768 , -0.22841033,  0.18651302,\n",
       "        0.5797427 , -0.24432805, -0.11956725, -0.486279  , -0.61172605,\n",
       "       -0.8216444 ,  0.3433727 ,  0.66347766, -0.97480774,  0.17584105,\n",
       "       -0.30671844,  0.33950254, -0.16996554,  0.14379308, -0.09252985,\n",
       "       -0.41257167,  0.20653144, -0.16734079, -0.359856  ,  0.5190938 ,\n",
       "       -0.95754546, -0.66358143,  0.1433572 ,  1.4758093 ,  0.22864471,\n",
       "        0.70676047,  0.7524492 ,  0.64946944,  0.4728902 ,  0.20559196,\n",
       "        0.6340505 ,  0.15475026,  0.00562241,  0.5084025 ,  0.68086576,\n",
       "        0.09820731,  0.3747469 , -0.55622697, -0.18547991, -1.1656444 ,\n",
       "       -0.3472246 , -0.45046   , -0.2884993 ,  0.17473376,  0.49551946,\n",
       "        0.4320302 , -0.05949261,  0.4697182 ,  0.5906235 ,  0.30922386,\n",
       "        0.05392113, -0.06284226,  0.09010233, -0.6635398 ,  0.6421263 ,\n",
       "       -0.82587916,  0.09375178,  0.25391847,  1.6995155 , -0.72286487,\n",
       "        0.72840977, -0.05969184, -0.7069017 ,  0.25710416, -0.3375597 ,\n",
       "        0.70582944,  1.3231224 ,  0.7797018 , -0.11657099,  0.40653896,\n",
       "       -0.4717132 , -0.83083475,  0.51853555,  0.06182001,  0.2746444 ,\n",
       "       -0.61381036,  0.13648461,  0.19157077,  0.97455347, -1.0007843 ,\n",
       "        0.7669219 , -0.2768041 , -0.4763001 ,  0.9759179 ,  0.81144726,\n",
       "       -0.18831532,  0.22462678,  0.02364958,  0.92605793,  0.48187006,\n",
       "       -1.4402837 ,  1.3045583 , -0.75270504,  0.3151865 , -0.67822886,\n",
       "       -0.45618463,  0.30720863,  0.282918  ,  0.00693694,  0.3464248 ,\n",
       "        0.34459233, -0.6466991 ,  0.21332431, -0.11703623,  1.6261065 ,\n",
       "        0.49894366, -0.5677027 , -1.0495661 , -0.2517564 , -0.8619844 ,\n",
       "       -0.39630094, -0.5423486 ,  0.03619807,  0.4566413 ,  0.54111457,\n",
       "       -0.73975825,  0.33861122, -0.5681545 , -0.7428227 ,  0.67170495,\n",
       "        0.611607  , -0.1860548 , -0.018881  , -0.16949132,  0.48770773,\n",
       "        0.1054076 ,  0.27332684,  0.5740291 , -1.0083718 , -0.5194535 ,\n",
       "       -0.06662448, -0.0111256 ,  0.16852376, -0.33837682, -0.18204994,\n",
       "       -0.30313948,  0.5307624 , -0.22126749, -0.59299815, -0.6133209 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained model tokenizer와 and bert model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # smaller & uncased model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bank가 들어간 유사한 문장 두 개\n",
    "sentence1 = \"I deposited money at the bank.\"\n",
    "sentence2 = \"The ducks swam to the river bank.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 BERT가 인식할 수 있는 형태로 Tokenize\n",
    "encoded_input1 = tokenizer(sentence1, return_tensors='pt')\n",
    "encoded_input2 = tokenizer(sentence2, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045, 14140,  2769,  2012,  1996,  2924,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `input_ids` : 각 단어별로 매핑된 key. 101은 문장의 시작을, 102는 문장의 끝을 의미\n",
    "- `token_type_ids` : 문장 번호\n",
    "- `attention_mask` : attention을 가져야 하는 단어는 1, 그렇지 않은 단어는 0. (만약 input이 실제 단어들이라면 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding 생성!\n",
    "with torch.no_grad():\n",
    "    output1 = model(**encoded_input1)\n",
    "    output2 = model(**encoded_input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding 내에서 bank라는 단어 찾아오기 (문장의 5번째에 있는 단어)\n",
    "bank_embedding_sentence1 = output1.last_hidden_state[0, 5, :]\n",
    "bank_embedding_sentence2 = output2.last_hidden_state[0, 5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between the two embeddings: tensor(0.5922)\n"
     ]
    }
   ],
   "source": [
    "# cosine similarity 계산을 통해 얼마나 유사한지 검증\n",
    "\n",
    "similarity = torch.nn.functional.cosine_similarity(bank_embedding_sentence1, bank_embedding_sentence2, dim=0)\n",
    "\n",
    "# print(\"Embedding for 'bank' in sentence 1:\", bank_embedding_sentence1)\n",
    "# print(\"Embedding for 'bank' in sentence 2:\", bank_embedding_sentence2)\n",
    "print(\"Cosine similarity between the two embeddings:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--END--"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
