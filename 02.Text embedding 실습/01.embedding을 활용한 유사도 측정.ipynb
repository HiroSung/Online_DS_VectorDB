{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec을 이용한 단어 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT야, simpsons 캐릭터 이름이 들어간 랜덤 문장 10개를 생성해줘\n",
    "\n",
    "sentences = [\"Homer Simpson forgot his lunch at home, so he had to buy a burger on his way to work.\",\n",
    "    \"Marge was busy knitting a new sweater for Bart's upcoming school play.\",\n",
    "    \"Lisa Simpson played a beautiful saxophone solo at the school concert.\",\n",
    "    \"Mr. Burns secretly plotted another scheme from his office at the Springfield Nuclear Power Plant.\",\n",
    "    \"Ned Flanders offered to help Homer fix the fence between their houses.\",\n",
    "    \"Bart Simpson tried a new prank at school, but it didn't go as planned.\",\n",
    "    \"Milhouse and Bart spent the afternoon playing video games and forgot to do their homework.\",\n",
    "    \"Maggie Simpson's adorable giggle filled the room as she played with her toys.\",\n",
    "    \"Apu had a busy day at the Kwik-E-Mart, dealing with a rush of customers.\",\n",
    "    \"Krusty the Clown decided to change his show a bit to attract a new audience.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "# get rid of stopwords, lower case\n",
    "\n",
    "sentences = [re.sub(r\"[.',]\", \"\", sentence).lower().split(\" \") for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word2vec\n",
    "\n",
    "skip_gram = Word2Vec(sentences, vector_size=300, min_count=1, window=5, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} 의 vector representation : \\n{}\".format('homer', skip_gram.wv.get_vector(skip_gram.wv.key_to_index['homer'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_gram.wv.most_similar(\"homer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직접 유사도 구해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homer_vector = skip_gram.wv.get_vector(skip_gram.wv.key_to_index['homer'])\n",
    "marge_vector = skip_gram.wv.get_vector(skip_gram.wv.key_to_index['marge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도 계산하기 from scratch\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vector_a: np.ndarray, vector_b: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    두 벡터간 cosine similarity를 계산\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vector_a : np.ndarray\n",
    "        The first input vector.\n",
    "    vector_b : np.ndarray\n",
    "        The second input vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The cosine similarity between `vector_a` and `vector_b`, which is a value between -1 and 1.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    dot_product = np.dot(vector_a, vector_b)\n",
    "    norm_a = norm(vector_a)\n",
    "    norm_b = norm(vector_b)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(homer_vector, marge_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simpsons dataset을 활용한 Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.edrawmax.com/what-is/simpsons-family-tree/example.png) <br>\n",
    "출처 : https://images.edrawmax.com/what-is/simpsons-family-tree/example.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('simpsons_dataset.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0, 'spoken_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize and remove the stopwords and non-alphabetic characters for each line of dialogue\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "\n",
    "def cleaning(doc):\n",
    "    \"\"\"\n",
    "    Cleans a spaCy Doc object by lemmatizing its tokens and removing stop words,\n",
    "    then joins the remaining tokens into a single string if there are more than two tokens left.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    doc : spacy.tokens.Doc\n",
    "        A spaCy Doc object containing the processed text.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    Optional : str\n",
    "        A string composed of the lemmatized, non-stop tokens separated by spaces,\n",
    "        if the resulting list of tokens has more than two elements. Otherwise, returns None.\n",
    "    \"\"\"\n",
    "\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep alphabets\n",
    "cleaner = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['spoken_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = [cleaning(doc) for doc in nlp.pipe(cleaner, batch_size=5000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe에 넣어서 null이 있는 대화는 삭제\n",
    "# 주로 null은 특정 행동을 했지만 대화가 없었을 때임\n",
    "\n",
    "df_clean = pd.DataFrame({'clean': txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 문장을 여러 단위의 단어로 분할\n",
    "sentences = [s.split(' ') for s in df_clean['clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `window` : 문장 내에서 현재 단어와 예측 단어 사이의 최대 거리. ex) 타겟 단어의 왼쪽과 오른쪽 n번째 단어\n",
    "- `vector_size` : 단어 벡터의 차원 수\n",
    "- `min_count` : 이 값보다 총 절대 빈도수가 낮은 모든 단어를 무시함 - (2, 100)\n",
    "- `sg` : 1은 skip-gram, 0은 CBOW method를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의 하기\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     vector_size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장에 들어있는 각 단어들을 Word2Vec 모델이 인식할 수 있는 형태로 변환\n",
    "w2v_model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(w2v_model.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어간 유사도 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(w2v_model.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- most_similar : 주어진 조건에 가장 적합한 단어 탐색\n",
    "- similarity : 주어진 단어들의 유사도 계산\n",
    "- doesnt_match : 주어진 단어들 중 가장 '덜 유사한' 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(w2v_model.wv.most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(w2v_model.wv.similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"homer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"bart\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Woman : homer = ___ : marge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"woman\", \"homer\"], negative=[\"marge\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"woman\", \"bart\"], negative=[\"man\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.doesnt_match(['bart', 'homer', 'marge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.doesnt_match(['bart', 'lisa', 'marge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 임베딩의 한계점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_vector = w2v_model.wv.get_vector(w2v_model.wv.key_to_index['bank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bank_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bank_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 우리가 사용하는 모든 단어는 context에 따라 의미가 다르다\n",
    "- 단어 embedding의 경우 이런 유연성을 확보하지 못 함\n",
    "    - 배를 깎아 먹었다 / 배가 고프다 / 배 멀미를 하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bank_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained model tokenizer와 and bert model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # smaller & uncased model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bank가 들어간 유사한 문장 두 개\n",
    "sentence1 = \"I deposited money at the bank.\"\n",
    "sentence2 = \"The ducks swam to the river bank.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 BERT가 인식할 수 있는 형태로 Tokenize\n",
    "encoded_input1 = tokenizer(sentence1, return_tensors='pt') # pytorch\n",
    "encoded_input2 = tokenizer(sentence2, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `input_ids` : 각 단어별로 매핑된 key. 101은 문장의 시작을, 102는 문장의 끝을 의미\n",
    "- `token_type_ids` : 문장 번호\n",
    "- `attention_mask` : attention을 가져야 하는 단어는 1, 그렇지 않은 단어는 0. (만약 input이 실제 단어들이라면 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding 생성!\n",
    "with torch.no_grad():\n",
    "    output1 = model(**encoded_input1)\n",
    "    output2 = model(**encoded_input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding 내에서 bank라는 단어 찾아오기 (문장의 5번째에 있는 단어)\n",
    "bank_embedding_sentence1 = output1.last_hidden_state[0, 5, :]\n",
    "bank_embedding_sentence2 = output2.last_hidden_state[0, 5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity 계산\n",
    "\n",
    "similarity = cosine_similarity(bank_embedding_sentence1, bank_embedding_sentence2)\n",
    "# print(\"Embedding for 'bank' in sentence 1:\", bank_embedding_sentence1)\n",
    "# print(\"Embedding for 'bank' in sentence 2:\", bank_embedding_sentence2)\n",
    "print(\"Cosine similarity between the two embeddings:\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--END--"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
